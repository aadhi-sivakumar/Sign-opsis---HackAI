ğŸ’¡ Inspiration
Sign-opsis was born from a desire to improve accessibility and communication for the deaf and hard of hearing community. We recognized the challenges in real-time sign language translationâ€”especially in fast-paced environments like virtual meetings, customer service interactions, and social media.

Our goal: Create a seamless bridge between spoken content and sign language.

ğŸ›  What It Does
ğŸ§ Input: Audio from videos, podcasts, or live captions

ğŸ¤Ÿ Output: A pair of animated hands performing American Sign Language (ASL)

ğŸ§± How We Built It
Speech Recognition (ASR):
We used OpenAIâ€™s Whisper to convert spoken language into text. Whisper handles various accents and background noise well. We added a preprocessing step to clean the text output before translation.

Text-to-ASL Translation:
English grammar doesnâ€™t directly map to ASL. We used spaCy to parse and restructure sentences into ASL-style gloss. Each word or letter is then mapped to corresponding hand coordinate data.

Datasets Used:

ASL Citizen â€“ Alphabet videos and metadata (CSV format)

WLASL â€“ A larger collection of glosses and metadata (JSON format)

Hand Animation:
To animate the hands:

We used OpenCV (cv2) to detect and generate point locations for each joint of the hand.

These points were connected to form a structured hand skeleton.

The points were then passed to MediaPipeâ€™s hand landmark model for frame-by-frame visualization.

This multi-step approach gave us greater control over how hand movements were visualized. However, aligning OpenCVâ€™s output with MediaPipeâ€™s landmark model was challenging, especially in the presence of noisy data or varying hand poses.

Final keypoints were stored in a structured coordinates.json, which can be played back to animate a 2D hand.

ğŸš§ Challenges We Faced
Managing Python and system requirements for spaCy, MediaPipe, and Whisper

Precisely aligning frame timing, pose estimation, and sign meaning

Handling Whisperâ€™s performance drop for audio files > 25MB

Ensuring smooth syncing of audio, sign output, and subtitles across the React + Flask stack

Resolving mismatch issues between OpenCVâ€™s keypoints and MediaPipeâ€™s landmark indexing

ğŸ† Accomplishments Weâ€™re Proud Of
Built a full hand joint extractor using OpenCV and MediaPipe

Maintained consistent (x, y) coordinate tracking across frames

Designed a clean, interactive frontend

Integrated Whisper to transcribe large audio files effectively with preprocessing and error handling

ğŸ“š What We Learned
Techniques for extracting and synchronizing 2D hand keypoints from both videos and static images

How to integrate audio processing, NLP, and computer vision into a modular backend

The linguistic differences between English and ASL, which are essential for accurate translation

ğŸš€ Whatâ€™s Next for Sign-opsis
ğŸŒ Support for multiple sign languages (e.g., BSL, ISL)

ğŸ˜€ Facial expression generation (key to expressive ASL)

ğŸ’» Integration with Zoom, YouTube, and Google Meet for real-time translation

ğŸ“¦ Offline support for low-connectivity areas

ğŸŒ Multilingual input support:

Convert audio/video in any language

Generate English transcript

Summarize content

Render 3D avatar performing sign language with synced text/audio subtitles
